
@article{acharyaExplainingCausalFindings2016,
  title = {Explaining {{Causal Findings Without Bias}}: {{Detecting}} and {{Assessing Direct Effects}}},
  shorttitle = {Explaining {{Causal Findings Without Bias}}},
  author = {Acharya, Avidit and Blackwell, Matthew and Sen, Maya},
  year = {2016},
  volume = {110},
  pages = {512--529},
  publisher = {{Cambridge University Press}},
  address = {{New York, USA}},
  issn = {0003-0554},
  doi = {10.1017/S0003055416000216},
  abstract = {Researchers seeking to establish causal relationships frequently control for variables on the purported causal pathway, checking whether the original treatment effect then disappears. Unfortunately, this common approach may lead to biased estimates. In this article, we show that the bias can be avoided by focusing on a quantity of interest called the controlled direct effect. Under certain conditions, the controlled direct effect enables researchers to rule out competing explanations\textemdash an important objective for political scientists. To estimate the controlled direct effect without bias, we describe an easy-to-implement estimation strategy from the biostatistics literature. We extend this approach by deriving a consistent variance estimator and demonstrating how to conduct a sensitivity analysis. Two examples\textemdash one on ethnic fractionalization's effect on civil war and one on the impact of historical plough use on contemporary female political participation\textemdash illustrate the framework and methodology.},
  file = {/Users/debaum/Zotero/storage/5KYMEDJX/ACHARYA et al. - 2016 - Explaining Causal Findings Without Bias Detecting.pdf},
  journal = {The American Political Science Review},
  keywords = {Analysis,Causation,Research bias},
  language = {eng},
  number = {3}
}

@article{atheyApproximateResidualBalancing2018,
  title = {Approximate {{Residual Balancing}}: {{Debiased Inference}} of {{Average Treatment Effects}} in {{hHgh Dimensions}}},
  shorttitle = {Approximate Residual Balancing},
  author = {Athey, Susan and Imbens, Guido W. and Wager, Stefan},
  year = {2018},
  volume = {80},
  pages = {597--623},
  publisher = {{Wiley Subscription Services, Inc}},
  issn = {1369-7412},
  doi = {10.1111/rssb.12268},
  abstract = {Summary There are many settings where researchers are interested in estimating average treatment effects and are willing to rely on the unconfoundedness assumption, which requires that the treatment assignment be as good as random conditional on pretreatment variables. The unconfoundedness assumption is often more plausible if a large number of pretreatment variables are included in the analysis, but this can worsen the performance of standard approaches to treatment effect estimation. We develop a method for debiasing penalized regression adjustments to allow sparse regression methods like the lasso to be used for {$\surd$}n-consistent inference of average treatment effects in high dimensional linear models. Given linearity, we do not need to assume that the treatment propensities are estimable, or that the average treatment effect is a sparse contrast of the outcome model parameters. Rather, in addition to standard assumptions used to make lasso regression on the outcome model consistent under 1-norm error, we require only overlap, i.e. that the propensity score be uniformly bounded away from 0 and 1. Procedurally, our method combines balancing weights with a regularized regression adjustment.},
  file = {/Users/debaum/Zotero/storage/6DRP78KB/Athey et al. - 2018 - Approximate residual balancing debiased inference.pdf},
  journal = {Journal of the Royal Statistical Society. Series B, Statistical Methodology},
  keywords = {Causal inference,Potential outcomes,Propensity score,Sparse estimation},
  language = {eng},
  number = {4}
}

@article{blackwellFrameworkDynamicCausal2013,
  title = {A {{Framework}} for {{Dynamic Causal Inference}} in {{Political Science}}},
  author = {Blackwell, Matthew},
  year = {2013},
  volume = {57},
  pages = {504--520},
  publisher = {{Blackwell Publishing Inc}},
  address = {{Malden, USA}},
  issn = {0092-5853},
  doi = {10.1111/j.1540-5907.2012.00626.x},
  abstract = {Dynamic strategies are an essential part of politics. In the context of campaigns, for example, candidates continuously recalibrate their campaign strategy in response to polls and opponent actions. Traditional causal inference methods, however, assume that these dynamic decisions are made all at once, an assumption that forces a choice between omitted variable bias and posttreatment bias. Thus, these kinds of "single-shot" causal inference methods are inappropriate for dynamic processes like campaigns. I resolve this dilemma by adapting methods from biostatistics, thereby presenting a holistic framework for dynamic causal inference. I then use this method to estimate the effectiveness of an inherently dynamic process: a candidate's decision to "go negative." Drawing on U.S. statewide elections (2000\textendash 2006), I find, in contrast to the previous literature and alternative methods, that negative advertising is an effective strategy for nonincumbents. I also describe a set of diagnostic tools and an approach to sensitivity analysis.},
  journal = {American Journal of Political Science},
  keywords = {Advertising campaigns,AJPS WORKSHOP,Analysis,Estimation methods,Estimators,Incumbents,Inference,Political campaigns,Political candidates,Political science,Polls,Term weighting,Voting},
  language = {eng},
  number = {2}
}

@article{coleConstructingInverseProbability2008,
  title = {Constructing {{Inverse Probability Weights}} for {{Marginal Structural Models}}},
  author = {Cole, Stephen R. and Hern{\'a}n, Miguel A.},
  year = {2008},
  volume = {168},
  pages = {656--664},
  publisher = {{Oxford University Press}},
  address = {{Cary, NC}},
  issn = {0002-9262},
  doi = {10.1093/aje/kwn164},
  abstract = {The method of inverse probability weighting (henceforth, weighting) can be used to adjust for measured confounding and selection bias under the four assumptions of consistency, exchangeability, positivity, and no misspecification of the model used to estimate weights. In recent years, several published estimates of the effect of time-varying exposures have been based on weighted estimation of the parameters of marginal structural models because, unlike standard statistical methods, weighting can appropriately adjust for measured time-varying confounders affected by prior exposure. As an example, the authors describe the last three assumptions using the change in viral load due to initiation of antiretroviral therapy among 918 human immunodeficiency virus-infected US men and women followed for a median of 5.8 years between 1996 and 2005. The authors describe possible tradeoffs that an epidemiologist may encounter when attempting to make inferences. For instance, a tradeoff between bias and precision is illustrated as a function of the extent to which confounding is controlled. Weight truncation is presented as an informal and easily implemented method to deal with these tradeoffs. Inverse probability weighting provides a powerful methodological tool that may uncover causal effects of exposures that are otherwise obscured. However, as with all methods, diagnostics and sensitivity analyses are essential for proper use.},
  file = {/Users/debaum/Zotero/storage/8P74R944/Cole and Hernán - 2008 - Constructing Inverse Probability Weights for Margi.pdf},
  journal = {American Journal of Epidemiology},
  keywords = {Acquired Immunodeficiency Syndrome - drug therapy,Acquired Immunodeficiency Syndrome - epidemiology,Analysis. Health state,Antiretroviral Therapy; Highly Active,Bias,bias (epidemiology),Bias (Statistics),Biological and medical sciences,causality,Confounding factors,confounding factors (epidemiology),Confounding Factors (Epidemiology),Epidemiology,Female,General aspects,HIV-1,Humans,Logistic Models,Male,Medical sciences,Methods,Miscellaneous,Multicenter Studies as Topic,Practice of Epidemiology,Probability,probability weighting,Public health. Hygiene,Public health. Hygiene-occupational medicine,Regression analysis,regression model,Research},
  language = {eng},
  number = {6}
}

@article{coleConstructingInverseProbability2008a,
  title = {Constructing {{Inverse Probability Weights}} for {{Marginal Structural Models}}},
  author = {Cole, Stephen R. and Hern{\'a}n, Miguel A.},
  year = {2008},
  volume = {168},
  pages = {656--664},
  publisher = {{Oxford University Press}},
  address = {{CARY}},
  issn = {0002-9262},
  doi = {10.1093/aje/kwn164},
  abstract = {The method of inverse probability weighting (henceforth, weighting) can be used to adjust for measured confounding and selection bias under the four assumptions of consistency, exchangeability, positivity, and no misspecification of the model used to estimate weights. In recent years, several published estimates of the effect of time-varying exposures have been based on weighted estimation of the parameters of marginal structural models because, unlike standard statistical methods, weighting can appropriately adjust for measured time-varying confounders affected by prior exposure. As an example, the authors describe the last three assumptions using the change in viral load due to initiation of antiretroviral therapy among 918 human immunodeficiency virus-infected US men and women followed for a median of 5.8 years between 1996 and 2005. The authors describe possible tradeoffs that an epidemiologist may encounter when attempting to make inferences. For instance, a tradeoff between bias and precision is illustrated as a function of the extent to which confounding is controlled. Weight truncation is presented as an informal and easily implemented method to deal with these tradeoffs. Inverse probability weighting provides a powerful methodological tool that may uncover causal effects of exposures that are otherwise obscured. However, as with all methods, diagnostics and sensitivity analyses are essential for proper use.},
  file = {/Users/debaum/Zotero/storage/CKJ6XEA9/Cole and Hernán - 2008 - Constructing Inverse Probability Weights for Margi.pdf},
  journal = {American Journal of Epidemiology},
  keywords = {Acquired Immunodeficiency Syndrome - drug therapy,Acquired Immunodeficiency Syndrome - epidemiology,Analysis. Health state,Antiretroviral Therapy; Highly Active,Bias,bias (epidemiology),Bias (Statistics),Biological and medical sciences,causality,Confounding factors,confounding factors (epidemiology),Confounding Factors (Epidemiology),Epidemiology,Female,General aspects,HIV-1,Humans,Life Sciences \& Biomedicine,Logistic Models,Male,Medical sciences,Methods,Miscellaneous,Multicenter Studies as Topic,Practice of Epidemiology,Probability,probability weighting,Public health. Hygiene,Public health. Hygiene-occupational medicine,Public; Environmental \& Occupational Health,Regression analysis,regression model,Research,Science \& Technology},
  language = {eng},
  number = {6}
}

@article{creamerSelfReportingRegimesMatter2019,
  title = {Do {{Self}}-{{Reporting Regimes Matter}}? {{Evidence}} from the {{Convention Against Torture}}},
  shorttitle = {Do {{Self}}-{{Reporting Regimes Matter}}?},
  author = {Creamer, Cosette D. and Simmons, Beth A.},
  year = {2019},
  volume = {63},
  pages = {1051--1064},
  publisher = {{Oxford University Press}},
  issn = {0020-8833},
  doi = {10.1093/isq/sqz043},
  abstract = {Abstract International regulatory agreements depend largely on self-reporting for implementation, yet we know almost nothing about whether or how such mechanisms work. We theorize that self-reporting processes provide information for domestic constituencies, with the potential to create pressure for better compliance. Using original data on state reports submitted to the Committee Against Torture, we demonstrate the influence of this process on the pervasiveness of torture and inhumane treatment. We illustrate the power of self-reporting regimes to mobilize domestic politics through evidence of civil society participation in shadow reporting, media attention, and legislative activity around antitorture law and practice. This is the first study to evaluate systematically the effects of self-reporting in the context of a treaty regime on human rights outcomes. Since many international agreements rely predominantly on self-reporting, the results have broad significance for compliance with international regulatory regimes globally.},
  file = {/Users/debaum/Zotero/storage/8DIGI94T/Creamer and Simmons - 2019 - Do Self-Reporting Regimes Matter Evidence from th.pdf},
  journal = {International Studies Quarterly},
  language = {eng},
  number = {4}
}

@incollection{elwertGraphicalCausalModels2013,
  title = {Graphical {{Causal Models}}},
  booktitle = {Handbook of {{Causal Analysis}} for {{Social Research}}},
  author = {Elwert, Felix},
  editor = {Morgan, Stephen L.},
  year = {2013},
  pages = {245--273},
  publisher = {{Springer}},
  address = {{Dordrecht}}
}

@misc{fongCBPSCovariateBalancing2021a,
  title = {{{CBPS}}: {{Covariate Balancing Propensity Score}}},
  shorttitle = {{{CBPS}}},
  author = {Fong, Christian and Ratkovic, Marc and Imai, Kosuke and Hazlett, Chad and Yang, Xiaolin and Peng, Sida},
  year = {2021},
  month = mar,
  abstract = {Implements the covariate balancing propensity score (CBPS) proposed by Imai and Ratkovic (2014) {$<$}doi:10.1111/rssb.12027{$>$}. The propensity score is estimated such that it maximizes the resulting covariate balance as well as the prediction of treatment assignment. The method, therefore, avoids an iteration between model fitting and balance checking. The package also implements optimal CBPS from Fan et al. (2016) {$<$}https://imai.fas.harvard.edu/research/CBPStheory.html{$>$}, several extensions of the CBPS beyond the cross-sectional, binary treatment setting. They include the CBPS for longitudinal settings so that it can be used in conjunction with marginal structural models from Imai and Ratkovic (2015) {$<$}doi:10.1080/01621459.2014.956872{$>$}, treatments with three- and four-valued treatment variables, continuous-valued treatments from Fong, Hazlett, and Imai (2018) {$<$}doi:10.1214/17-AOAS1101{$>$}, propensity score estimation with a large number of covariates from Ning, Peng, and Imai (2018) {$<$}arXiv:1812.08683{$>$}, and the situation with multiple distinct binary treatments administered simultaneously. In the future it will be extended to other settings including the generalization of experimental and instrumental variable estimates.},
  copyright = {GPL-2 | GPL-3 [expanded from: GPL ({$\geq$} 2)]}
}

@article{fongCovariateBalancingPropensity2018,
  title = {Covariate Balancing Propensity Score for a Continuous Treatment: {{Application}} to the Efficacy of Political Advertisements},
  shorttitle = {Covariate Balancing Propensity Score for a Continuous Treatment},
  author = {Fong, Christian and Hazlett, Chad and Imai, Kosuke},
  year = {2018},
  volume = {12},
  issn = {1932-6157},
  doi = {10.1214/17-AOAS1101},
  file = {/Users/debaum/Zotero/storage/3KISA5U3/Fong et al. - 2018 - Covariate balancing propensity score for a continu.pdf},
  journal = {The Annals of Applied Statistics},
  language = {eng},
  number = {1}
}

@article{hainmuellerEntropyBalancingCausal2012,
  title = {Entropy {{Balancing}} for {{Causal Effects}}: {{A Multivariate Reweighting Method}} to {{Produce Balanced Samples}} in {{Observational Studies}}},
  shorttitle = {Entropy {{Balancing}} for {{Causal Effects}}},
  author = {Hainmueller, Jens},
  year = {2012},
  volume = {20},
  pages = {25--46},
  publisher = {{Cambridge University Press}},
  address = {{New York, US}},
  issn = {1047-1987},
  doi = {10.1093/pan/mpr025},
  abstract = {This paper proposes entropy balancing, a data preprocessing method to achieve covariate balance in observational studies with binary treatments. Entropy balancing relies on a maximum entropy reweighting scheme that calibrates unit weights so that the reweighted treatment and control group satisfy a potentially large set of prespecified balance conditions that incorporate information about known sample moments. Entropy balancing thereby exactly adjusts inequalities in representation with respect to the first, second, and possibly higher moments of the covariate distributions. These balance improvements can reduce model dependence for the subsequent estimation of treatment effects. The method assures that balance improves on all covariate moments included in the reweighting. It also obviates the need for continual balance checking and iterative searching over propensity score models that may stochastically balance the covariate moments. We demonstrate the use of entropy balancing with Monte Carlo simulations and empirical applications.},
  file = {/Users/debaum/Zotero/storage/2FVT39R7/Hainmueller - 2012 - Entropy Balancing for Causal Effects A Multivaria.pdf},
  journal = {Political Analysis},
  keywords = {Control groups,Control units,Entropy,Estimation methods,Estimators,Mathematical moments,Modeling,Observational studies,Preprocessing,Term weighting},
  language = {eng},
  number = {1}
}

@article{imaiCovariateBalancingPropensity2014,
  title = {Covariate {{Balancing Propensity Score}}},
  author = {Imai, Kosuke and Ratkovic, Marc},
  year = {2014},
  volume = {76},
  pages = {243--263},
  publisher = {{Blackwell Publishing Ltd}},
  issn = {1369-7412},
  doi = {10.1111/rssb.12027},
  abstract = {The propensity score plays a central role in a variety of causal inference settings. In particular, matching and weighting methods based on the estimated propensity score have become increasingly common in the analysis of observational data. Despite their popularity and theoretical appeal, the main practical difficulty of these methods is that the propensity score must be estimated. Researchers have found that slight misspecification of the propensity score model can result in substantial bias of estimated treatment effects. We introduce covariate balancing propensity score (CBPS) methodology, which models treatment assignment while optimizing the covariate balance. The CBPS exploits the dual characteristics of the propensity score as a covariate balancing score and the conditional probability of treatment assignment. The estimation of the CBPS is done within the generalized method-of-moments or empirical likelihood framework. We find that the CBPS dramatically improves the poor empirical performance of propensity score matching and weighting methods reported in the literature. We also show that the CBPS can be extended to other important settings, including the estimation of the generalized propensity score for non-binary treatments and the generalization of experimental estimates to a target population. Open source software is available for implementing the methods proposed.},
  file = {/Users/debaum/Zotero/storage/L9NRC9E4/Imai and Ratkovic - 2014 - Covariate balancing propensity score.pdf},
  journal = {Journal of the Royal Statistical Society. Series B, Statistical Methodology},
  keywords = {Analysis,Causal inference,Instrumental variables,Inverse propensity score weighting,Marginal structural models,Observational studies,Propensity score matching,Public software,Randomized experiments},
  language = {eng},
  number = {1}
}

@article{imaiIdentificationInferenceSensitivity2010,
  title = {Identification, {{Inference}} and {{Sensitivity Analysis}} for {{Causal Mediation Effects}}},
  author = {Imai, Kosuke and Keele, Luke and Yamamoto, Teppei},
  year = {2010},
  volume = {25},
  pages = {51--71},
  publisher = {{Institute of Mathematical Statistics}},
  address = {{Hayward}},
  issn = {0883-4237},
  doi = {10.1214/10-STS321},
  abstract = {Causal mediation analysis is routinely conducted by applied researchers in a variety of disciplines. The goal of such an analysis is to investigate alternative causal mechanisms by examining the roles of intermediate variables that lie in the causal paths between the treatment and outcome variables. In this paper we first prove that under a particular version of sequential ignorability assumption, the average causal mediation effect (ACME) is nonparametrically identified. We compare our identification assumption with those proposed in the literature. Some practical implications of our identification result are also discussed. In particular, the popular estimator based on the linear structural equation model (LSEM) can be interpreted as an ACME estimator once additional parametric assumptions are made. We show that these assumptions can easily be relaxed within and outside of the LSEM framework and propose simple nonparametric estimation strategies. Second, and perhaps most importantly, we propose a new sensitivity analysis that can be easily implemented by applied researchers within the LSEM framework. Like the existing identifying assumptions, the proposed sequential ignorability assumption may be too strong in many applied settings. Thus, sensitivity analysis is essential in order to examine the robustness of empirical findings to the possible existence of an unmeasured confounder. Finally, we apply the proposed methods to a randomized experiment from political psychology. We also make easy-to-use software available to implement the proposed methods.},
  file = {/Users/debaum/Zotero/storage/JUS5YZQ3/Imai et al. - 2010 - Identification, Inference and Sensitivity Analysis.pdf},
  journal = {Statistical science},
  keywords = {Analytical estimating,Causal inference,causal mediation analysis,Confidence interval,direct and indirect effects,Epidemiology,Estimators,Freedom of speech,Generalized linear models,Inference,linear structural equation models,Logical givens,Parameter estimation,Political psychology,Sample size,Sensitivity analysis,sequential ignorability,Social sciences,Statistical inference,Statistical mechanics,Statistics,Statistics - Methodology,unmeasured confounders},
  language = {eng},
  number = {1}
}

@article{imaiRobustEstimationInverse2015,
  title = {Robust {{Estimation}} of {{Inverse Probability Weights}} for {{Marginal Structural Models}}},
  author = {Imai, Kosuke and Ratkovic, Marc},
  year = {2015},
  volume = {110},
  pages = {1013--1023},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2014.956872},
  abstract = {Marginal structural models (MSMs) are becoming increasingly popular as a tool for causal inference from longitudinal data. Unlike standard regression models, MSMs can adjust for time-dependent observed confounders while avoiding the bias due to the direct adjustment for covariates affected by the treatment. Despite their theoretical appeal, a main practical difficulty of MSMs is the required estimation of inverse probability weights. Previous studies have found that MSMs can be highly sensitive to misspecification of treatment assignment model even when the number of time periods is moderate. To address this problem, we generalize the covariate balancing propensity score (CBPS) methodology of Imai and Ratkovic to longitudinal analysis settings. The CBPS estimates the inverse probability weights such that the resulting covariate balance is improved. Unlike the standard approach, the proposed methodology incorporates all covariate balancing conditions across multiple time periods. Since the number of these conditions grows exponentially as the number of time period increases, we also propose a low-rank approximation to ease the computational burden. Our simulation and empirical studies suggest that the CBPS significantly improves the empirical performance of MSMs by making the treatment assignment model more robust to misspecification. Open-source software is available for implementing the proposed methods.},
  file = {/Users/debaum/Zotero/storage/SWWF7UUX/Imai and Ratkovic - 2015 - Robust Estimation of Inverse Probability Weights f.pdf},
  journal = {Journal of the American Statistical Association},
  keywords = {Causal inference,Covariate balancing propensity score,Inverse propensity score weighting,Observational studies,Sequential ignorability,Theory and Methods,Time-dependent treatments},
  language = {eng},
  number = {511}
}

@article{imaiUnpackingBlackBox2011,
  title = {Unpacking the {{Black Box}} of {{Causality}}: {{Learning}} about {{Causal Mechanisms}} from {{Experimental}} and {{Observational Studies}}},
  shorttitle = {Unpacking the {{Black Box}} of {{Causality}}},
  author = {Imai, Kosuke and Keele, Luke and Tingley, Dustin and Yamamoto, Teppei},
  year = {2011},
  volume = {105},
  pages = {765--789},
  publisher = {{Cambridge University Press}},
  address = {{New York, USA}},
  issn = {0003-0554},
  doi = {10.1017/S0003055411000414},
  abstract = {Identifying causal mechanisms is a fundamental goal of social science. Researchers seek to study not only whether one variable affects another but also how such a causal relationship arises. Yet commonly used statistical methods for identifying causal mechanisms rely upon untestable assumptions and are often inappropriate even under those assumptions. Randomizing treatment and intermediate variables is also insufficient. Despite these difficulties, the study of causal mechanisms is too important to abandon. We make three contributions to improve research on causal mechanisms. First, we present a minimum set of assumptions required under standard designs of experimental and observational studies and develop a general algorithm for estimating causal mediation effects. Second, we provide a method for assessing the sensitivity of conclusions to potential violations of a key assumption. Third, we offer alternative research designs for identifying causal mechanisms under weaker assumptions. The proposed approach is illustrated using media framing experiments and incumbency advantage studies.},
  file = {/Users/debaum/Zotero/storage/THYI3VVV/IMAI et al. - 2011 - Unpacking the Black Box of Causality Learning abo.pdf},
  journal = {The American political science review},
  keywords = {Algorithms,Causation,Experiment design,Incumbents,Inference,Observational research,Observational studies,Political candidates,Pretreatment,Research,Research design,Sensitivity analysis,Voting},
  language = {eng},
  number = {4}
}

@article{imaiWhenShouldWe2019,
  title = {When {{Should We Use Unit Fixed Effects Regression Models}} for {{Causal Inference}} with {{Longitudinal Data}}?},
  author = {Imai, Kosuke and Kim, In Song},
  year = {2019},
  volume = {63},
  pages = {467--490},
  publisher = {{Wiley Subscription Services, Inc}},
  issn = {0092-5853},
  doi = {10.1111/ajps.12417},
  abstract = {Many researchers use unit fixed effects regression models as their default methods for causal inference with longitudinal data. We show that the ability of these models to adjust for unobserved time-invariant confounders comes at the expense of dynamic causal relationships, which are permitted under an alternative selection-on-observables approach. Using the nonparametric directed acyclic graph, we highlight two key causal identification assumptions of unit fixed effects models: Past treatments do not directly influence current outcome, and past outcomes do not affect current treatment. Furthermore, we introduce a new nonparametric matching framework that elucidates how various unit fixed effects models implicitly compare treated and control observations to draw causal inference. By establishing the equivalence between matching and weighted unit fixed effects estimators, this framework enables a diverse set of identification strategies to adjust for unobservables in the absence of dynamic causal relationships between treatment and outcome variables. We illustrate the proposed methodology through its application to the estimation of GATT membership effects on dyadic trade volume.},
  journal = {American Journal of Political Science},
  keywords = {AJPS WORKSHOP,Analysis,International trade,Models,Public software,Usage},
  language = {eng},
  number = {2}
}

@article{kangDemystifyingDoubleRobustness2007,
  title = {Demystifying {{Double Robustness}}: {{A Comparison}} of {{Alternative Strategies}} for {{Estimating}} a {{Population Mean}} from {{Incomplete Data}}},
  shorttitle = {Demystifying {{Double Robustness}}},
  author = {Kang, Joseph D. Y. and Schafer, Joseph L.},
  year = {2007},
  volume = {22},
  pages = {523--539},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237},
  doi = {10.1214/07-STS227},
  abstract = {When outcomes are missing for reasons beyond an investigator's control, there are two different ways to adjust a parameter estimate for covariates that may be related both to the outcome and to missingness. One approach is to model the relationships between the covariates and the outcome and use those relationships to predict the missing values. Another is to model the probabilities of missingness given the covariates and incorporate them into a weighted or stratified estimate. Doubly robust (DR) procedures apply both types of model simultaneously and produce a consistent estimate of the parameter if either of the two models has been correctly specified. In this article, we show that DR estimates can be constructed in many ways. We compare the performance of various DR and non-DR estimates of a population mean in a simulated example where both models are incorrect but neither is grossly misspecified. Methods that use inverse-probabilities as weights, whether they are DR or not, are sensitive to misspecification of the propensity model when some estimated propensities are small. Many DR methods perform better than simple inverse-probability weighting. None of the DR methods we tried, however, improved upon the performance of simple regression-based prediction of the missing values. This study does not represent every missing-data problem that will arise in practice. But it does demonstrate that, in at least some settings, two wrong models are not better than one.},
  file = {/Users/debaum/Zotero/storage/J42FJE4W/Kang and Schafer - 2007 - Demystifying Double Robustness A Comparison of Al.pdf},
  journal = {Statistical Science},
  keywords = {Causal inference,Consistent estimators,Estimation bias,Estimation methods,Estimators,Inference,missing data,model-assisted survey estimation,Modeling,Population estimates,Population mean,propensity score,Sampling bias,Statistics - Methodology,Term weighting,weighted estimating equations},
  language = {eng},
  number = {4}
}

@article{ladamProminentRoleModels2018,
  title = {Prominent {{Role Models}}: {{High}}-{{Profile Female Politicians}} and the {{Emergence}} of {{Women}} as {{Candidates}} for {{Public Office}}},
  shorttitle = {Prominent {{Role Models}}},
  author = {Ladam, Christina and Harden, Jeffrey J. and Windett, Jason H.},
  year = {2018},
  volume = {62},
  pages = {369--381},
  publisher = {{Wiley Subscription Services, Inc}},
  issn = {0092-5853},
  doi = {10.1111/ajps.12351},
  abstract = {Can prominent female politicians inspire other women to enter politics? A woman occupying a high-profile office directly impacts women's substantive representation through her policy actions. Here, we consider whether these female leaders also facilitate a mobilization effect by motivating other women to run for office. We posit that prominent women in politics serve as role models for other women interested in political careers, causing an increase in female candidates. We test this theory with data from the American states, which exhibit considerable variation in the sex of state legislative candidates and the high-profile offices of governor and U.S. senator. Using a weighting method and data spanning 1978\textendash 2012, we demonstrate that high-profile women exert substantively large positive effects on female candidates. We conclude that women in major offices are crucial for women's representation. Beyond their direct policy impact, they amplify women's political voice by motivating more women to enter politics.},
  journal = {American Journal of Political Science},
  keywords = {Political activity,Political aspects,Women executives,Women in politics,Women politicians},
  language = {eng},
  number = {2}
}

@article{lauEffectsNegativePolitical2007,
  title = {The {{Effects}} of {{Negative Political Campaigns}}: {{A Meta}}-{{Analytic Reassessment}}},
  shorttitle = {The {{Effects}} of {{Negative Political Campaigns}}},
  author = {Lau, Richard R. and Sigelman, Lee and Rovner, Ivy Brown},
  year = {2007},
  volume = {69},
  pages = {1176--1209},
  publisher = {{Cambridge University Press}},
  address = {{New York, USA}},
  issn = {0022-3816},
  doi = {10.1111/j.1468-2508.2007.00618.x},
  abstract = {The conventional wisdom about negative political campaigning holds that it works, i.e., it has the consequences its practitioners intend. Many observers also fear that negative campaigning has unintended but detrimental effects on the political system itself. An earlier meta-analytic assessment of the relevant literature found no reliable evidence for these claims, but since then the research literature has more than doubled in size and has greatly improved in quality. We reexamine this literature and find that the major conclusions from the earlier meta-analysis still hold. All told, the research literature does not bear out the idea that negative campaigning is an effective means of winning votes, even though it tends to be more memorable and stimulate knowledge about the campaign. Nor is there any reliable evidence that negative campaigning depresses voter turnout, though it does slightly lower feelings of political efficacy, trust in government, and possibly overall public mood.},
  journal = {The Journal of Politics},
  keywords = {Analysis,Campaign literature,Electioneering,Null hypothesis,Political campaigns,Political candidates,Political systems,Sample size,Sampling errors,Statistical significance,Voter turnout,Voting},
  language = {eng},
  number = {4}
}

@article{naimiConstructingInverseProbability2014,
  title = {Constructing {{Inverse Probability Weights}} for {{Continuous Exposures}}: {{A Comparison}} of {{Methods}}},
  shorttitle = {Constructing {{Inverse Probability Weights}} for {{Continuous Exposures}}},
  author = {Naimi, Ashley I. and Moodie, Erica EM and Auger, Nathalie and Kaufman, Jay S.},
  year = {2014},
  volume = {25},
  pages = {292--299},
  publisher = {{Lippincott Williams \& Wilkins, Inc}},
  address = {{Philadelphia, PA}},
  issn = {1044-3983},
  doi = {10.1097/EDE.0000000000000053},
  abstract = {Inverse probability\textendash weighted marginal structural models with binary exposures are common in epidemiology. Constructing inverse probability weights for a continuous exposure can be complicated by the presence of outliers, and the need to identify a parametric form for the exposure and account for nonconstant exposure variance. We explored the performance of various methods to construct inverse probability weights for continuous exposures using Monte Carlo simulation. We generated two continuous exposures and binary outcomes using data sampled from a large empirical cohort. The first exposure followed a normal distribution with homoscedastic variance. The second exposure followed a contaminated Poisson distribution, with heteroscedastic variance equal to the conditional mean. We assessed six methods to construct inverse probability weights using: a normal distribution, a normal distribution with heteroscedastic variance, a truncated normal distribution with heteroscedastic variance, a gamma distribution, a t distribution (1,3, and 5 degrees of freedom), and a quantile binning approach (based on 10, 15, and 20 exposure categories). We estimated the marginal odds ratio for a single-unit increase in each simulated exposure in a regression model weighted by the inverse probability weights constructed using each approach, and then computed the bias and mean squared error for each method. For the homoscedastic exposure, the standard normal, gamma, and quantile binning approaches performed best. For the heteroscedastic exposure, the quantile binning, gamma, and heteroscedastic normal approaches performed best. Our results suggest that the quantile binning approach is a simple and versatile way to construct inverse probability weights for continuous exposures.},
  journal = {Epidemiology (Cambridge, Mass.)},
  keywords = {Biological and medical sciences,Birth,Birth Certificates,Computer Simulation,Data Interpretation; Statistical,Degrees of freedom,Disease models,Epidemiologic Research Design,Epidemiology,Estimation methods,Female,Gaussian distributions,General aspects,Humans,Infant; Newborn,Medical sciences,Methods,Miscellaneous,Models; Statistical,Monte Carlo Method,Odds Ratio,Poisson Distribution,Pregnancy,Probability,Public health. Hygiene,Public health. Hygiene-occupational medicine,Quebec,Regression analysis,Simulations,Statistical variance,T distribution},
  language = {eng},
  number = {2}
}

@article{neweyAsymptoticVarianceSemiparametric1994,
  title = {The {{Asymptotic Variance}} of {{Semiparametric Estimators}}},
  author = {Newey, Whitney K.},
  year = {1994},
  volume = {62},
  pages = {1349--1382},
  publisher = {{Econometric Society}},
  address = {{Menasha, Wis}},
  issn = {0012-9682},
  doi = {10.2307/2951752},
  abstract = {The purpose of this paper is the presentation of a general formula for the asymptotic variance of a semiparametric estimator. A particularly important feature of this formula is a way of accounting for the presence of nonparametric estimates of nuisance functions. The general form of an adjustment factor for nonparametric estimates is derived and analyzed. The usefulness of the formula is illustrated by deriving propositions on invariance of the limiting distribution with respect to the nonparametric estimator, conditions for nonparametric estimation to have no effect on the asymptotic distribution, and the form of a correction term for the presence of nonparametric projection and density estimators. Examples discussed are quasi-maximum likelihood estimation of index models, panel probit with semiparametric individual effects, average derivatives, and inverse density weighted least squares. The paper also develops a set of regularity conditions for the validity of the asymptotic variance formula. Primitive regularity conditions are derived for \$\textbackslash sqrt\{n\}\textbackslash text\{-consistency\}\$ and asymptotic normality for functions of series estimators of projections. Specific examples are polynomial estimators of average derivative and semiparametric panel probit models.},
  file = {/Users/debaum/Zotero/storage/2UL38AK3/Newey - 1994 - The Asymptotic Variance of Semiparametric Estimato.pdf},
  journal = {Econometrica},
  keywords = {Analysis,Approximation,Asymptotic distribution (Probability theory),Consistent estimators,Density estimation,Estimation theory,Estimators,Least squares,Mathematical functions,Methods,Parameter estimation,Polynomials,Series convergence,Statistical variance},
  language = {eng},
  number = {6}
}

@book{pearlCausalityModelsReasoning2009,
  title = {Causality: {{Models}}, {{Reasoning}}, and {{Inference}}},
  shorttitle = {Causality},
  author = {Pearl, Judea},
  year = {2009},
  edition = {2nd ed.},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge ; New York}},
  annotation = {HOLLIS number: 990120440720203941},
  isbn = {978-0-521-89560-6},
  keywords = {Causation,Probabilities},
  language = {eng},
  lccn = {BD541 .P43 2009, QA 273 P359c 2009}
}

@inproceedings{pearlDirectIndirectEffects,
  title = {Direct and {{Indirect Effects}}},
  booktitle = {Proceedings of the {{Seventeenth Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Pearl, Judea},
  pages = {411--420},
  publisher = {{Morgan Kaufmann Publishers Inc}},
  address = {{San Francisco, CA}}
}

@article{robinsMarginalStructuralModels2000,
  title = {Marginal {{Structural Models}} and {{Causal Inference}} in {{Epidemiology}}},
  author = {Robins, James M. and Hernan, Miguel A. and Brumback, Babette},
  year = {2000},
  volume = {11},
  pages = {550--560},
  publisher = {{Lippincott Williams \& Wilkins}},
  address = {{Philadelphia, PA}},
  issn = {1044-3983},
  doi = {10.1097/00001648-200009000-00011},
  abstract = {In observational studies with exposures or treatments that vary over time, standard approaches for adjustment of confounding are biased when there exist time-dependent confounders that are also affected by previous treatment. This paper introduces marginal structural models, a new class of causal models that allow for improved adjustment of confounding in those situations. The parameters of a marginal structural model can be consistently estimated using a new class of estimators, the inverse-probability-of-treatment weighted estimators.},
  journal = {Epidemiology},
  keywords = {Anti-HIV Agents - therapeutic use,Biological and medical sciences,Causality,Censorship,Confounding Factors (Epidemiology),Disease models,Epidemiologic Methods,Epidemiology,Estimators,General aspects,HIV Infections - drug therapy,HIV Infections - mortality,Humans,Inference,Logistic regression,Logistics,Medical sciences,Methodology,Models; Statistical,Parametric models,Predisposing factors,Public health. Hygiene,Public health. Hygiene-occupational medicine,Risk Factors,Time Factors,Unbiased estimators,Zidovudine - therapeutic use},
  language = {eng},
  number = {5}
}

@incollection{robinsMarginalStructuralModels2000a,
  title = {Marginal {{Structural Models}} versus {{Structural Nested Models}} as {{Tools}} for {{Causal Inference}}},
  booktitle = {Statistical {{Models}} in {{Epidemiology}}, the {{Environment}}, and {{Clinical Trials}}},
  author = {Robins, James M.},
  editor = {Halloran, Elizabeth M. and Berry, Donald},
  year = {2000},
  pages = {95--133},
  publisher = {{Springer}},
  address = {{New York}}
}

@article{robinsNewApproachCausal1986,
  title = {A {{New Approach}} to {{Causal Inference}} in {{Mortality Studies}} with a {{Sustained Exposure Period}} \textemdash{} {{Application}} to {{Control}} of the {{Healthy Worker Survivor Effect}}},
  author = {Robins, James M.},
  year = {1986},
  volume = {7},
  pages = {1393--1512},
  publisher = {{Elsevier BV}},
  issn = {0270-0255},
  doi = {10.1016/0270-0255(86)90088-6},
  abstract = {In observational cohort mortality studies with prolonged periods of exposure to the agent under study, it is not uncommon for risk factors for death to be determinants of subsequent exposure. For instance, in occupational mortality studies date of termination of employment is both a determinant of future exposure (since terminated individuals receive no further exposure) and an independent risk factor for death (since disabled individuals tend to leave employment). When current risk factor status determines subsequent exposure and is determined by previous exposure, standard analyses that estimate age-specific mortality rates as a function of cumulative exposure may underestimate the true effect of exposure on mortality whether or not one adjusts for the risk factor in the analysis. This observation raises the question, which if any population parameters can be given a causal interpretation in observational mortality studies? In answer, we offer a graphical approach to the identification and computation of causal parameters in mortality studies with sustained exposure periods. This approach is shown to be equivalent to an approach in which the observational study is identified with a hypothetical double-blind randomized trial in which data on each subject's assigned treatment protocol has been erased from the data file. Causal inferences can then be made by comparing mortality as a function of treatment protocol, since, in a double-blind randomized trial missing data on treatment protocol, the association of mortality with treatment protocol can still be estimated. We reanalyze the mortality experience of a cohort of arsenic-exposed copper smelter workers with our method and compare our results with those obtained using standard methods. We find an adverse effect of arsenic exposure on all-cause and lung cancer mortality which standard methods fail to detect.},
  journal = {Mathematical Modelling},
  language = {eng},
  number = {9}
}

@incollection{robinsSemanticsCausalDAG2003,
  title = {Semantics of {{Causal DAG}} Models and the {{Identification}} of {{Direct}} and {{Indirect Effects}}},
  booktitle = {Highly {{Structured Stochastic Systems}}},
  author = {Robins, James M.},
  editor = {Green, Peter J. and Hjort, Nils L. and Richardson, Sylvia},
  year = {2003},
  pages = {70--81},
  number = {27},
  series = {Oxford {{Statistical Science Series}}}
}

@article{tomzPublicOpinionDemocratic2013a,
  title = {Public {{Opinion}} and the {{Democratic Peace}}},
  author = {Tomz, Michael R. and Weeks, Jessica L. P.},
  year = {2013},
  volume = {107},
  pages = {849--865},
  publisher = {{Cambridge University Press}},
  address = {{New York, USA}},
  issn = {0003-0554},
  doi = {10.1017/S0003055413000488},
  abstract = {One of the most striking findings in political science is the democratic peace: the absence of war between democracies. Some authors attempt to explain this phenomenon by highlighting the role of public opinion. They observe that democratic leaders are beholden to voters and argue that voters oppose war because of its human and financial costs. This logic predicts that democracies should behave peacefully in general, but history shows that democracies avoid war primarily in their relations with other democracies. In this article we investigate not whether democratic publics are averse to war in general, but whether they are especially reluctant to fight other democracies. We embedded experiments in public opinion polls in the United States and the United Kingdom and found that individuals are substantially less supportive of military strikes against democracies than against otherwise identical autocracies. Moreover, our experiments suggest that shared democracy pacifies the public primarily by changing perceptions of threat and morality, not by raising expectations of costs or failure. These findings shed light on a debate of enduring importance to scholars and policy makers.},
  journal = {The American Political Science Review},
  keywords = {Autocracy,Democracy,Democratic theory,Foreign policy,Influence,Military alliances,Military aspects,Nuclear weapons,Peacetime,Political aspects,Public opinion,Security; International,Statistical significance,United Kingdom,United States,War},
  language = {eng},
  number = {4}
}

@article{urbanDollarsSidewalkShould2014a,
  title = {Dollars on the {{Sidewalk}}: {{Should U}}.{{S}}. {{Presidential Candidates Advertise}} in {{Uncontested States}}?},
  shorttitle = {Dollars on the {{Sidewalk}}},
  author = {Urban, Carly and Niebler, Sarah},
  year = {2014},
  volume = {58},
  pages = {322--336},
  publisher = {{Blackwell Publishing Ltd}},
  issn = {0092-5853},
  doi = {10.1111/ajps.12073},
  abstract = {Presidential candidates in the United States do not intentionally advertise in states without rigorous competition for electoral votes. However, in some areas of noncompetitive states, media markets overlap with battleground states, exposing these regions to political ads. These spillover advertisements allow us to examine the relationship between advertisements and individual campaign contributions, with data from the Wisconsin Advertising Project and the Federal Elections Commission. Using propensity-score matching within uncontested states, we find that 2008 aggregate giving in zip codes exposed to political ads was approximately \$6,100 (28.1\% of mean contributions) more than in similar zip codes without advertisements.},
  journal = {American Journal of Political Science},
  keywords = {Advertising campaigns,Analysis,Area of dominant influence,Campaign contributions,Political advertising,Political campaigns,Political candidates,Presidential candidates,Presidential elections,State elections,ZIP codes},
  language = {eng},
  number = {2}
}

@article{vanderwalIpwPackageInverse2011,
  title = {Ipw: {{An R Package}} for {{Inverse Probability Weighting}}},
  shorttitle = {Ipw},
  author = {{van der Wal}, Willem M. and Geskus, Ronald B.},
  year = {2011},
  volume = {43},
  pages = {1--23},
  publisher = {{JOURNAL STATISTICAL SOFTWARE}},
  address = {{LOS ANGELES}},
  issn = {1548-7660},
  abstract = {We describe the R package ipw for estimating inverse probability weights. We show how to use the package to fit marginal structural models through inverse probability weighting, to estimate causal effects. Our package can be used with data from a point treatment situation as well as with a time-varying exposure and time-varying confounders. It can be used with binomial, categorical, ordinal and continuous exposure variable.},
  journal = {Journal of Statistical Software},
  keywords = {Computer Science,Computer Science; Interdisciplinary Applications,Mathematics,Physical Sciences,Science \& Technology,Statistics \& Probability,Technology},
  language = {eng},
  number = {13}
}

@book{vanderweeleExplanationCausalInference2015,
  title = {Explanation in {{Causal Inference}}: {{Methods}} for {{Mediation}} and {{Interaction}}},
  shorttitle = {Explanation in Causal Inference},
  author = {VanderWeele, Tyler},
  year = {2015},
  publisher = {{Oxford University Press}},
  address = {{New York}},
  abstract = {The book provides an accessible but comprehensive overview of methods for mediation and interaction. There has been considerable and rapid methodological development on mediation and moderation/interaction analysis within the causal-inference literature over the last ten years. Much of this material appears in a variety of specialized journals, and some of the papers are quite technical. There has also been considerable interest in these developments from empirical researchers in the social and biomedical sciences. However, much of the material is not currently in a format that is accessible t},
  isbn = {978-0-19-932588-7},
  keywords = {Causation,Methodology,Research,Social sciences},
  language = {eng}
}

@article{vansteelandtEstimatingDirectEffects2009,
  title = {Estimating {{Direct Effects}} in {{Cohort}} and {{Case}}\textendash{{Control Studies}}},
  author = {Vansteelandt, Stijn},
  year = {2009},
  volume = {20},
  pages = {851--860},
  publisher = {{Lippincott Williams \& Wilkins}},
  address = {{Philadelphia, PA}},
  issn = {1044-3983},
  doi = {10.1097/EDE.0b013e3181b6f4c9},
  abstract = {Estimating the effect of an exposure on an outcome, other than through some given mediator, requires adjustment for all risk factors of the mediator that are also associated with the outcome. When these risk factors are themselves affected by the exposure, then standard regression methods do not apply. In this article, I review methods for accommodating this and discuss their limitations for estimating the controlled direct effect (ie, the exposure effect when controlling the mediator at a specified level uniformly in the population). In addition, I propose a powerful and easy-to-apply alternative that uses G-estimation in structural nested models to address these limitations both for cohort and case\textendash control studies.},
  journal = {Epidemiology (Cambridge, Mass.)},
  keywords = {Alcohol drinking,Biological and medical sciences,Case control studies,Causality,Cohort Studies,Confounding Factors (Epidemiology),Epidemiologic Research Design,Epidemiology,Estimators,General aspects,Health outcomes,Humans,Least squares,Medical sciences,METHODS,Miscellaneous,Modeling,Models; Statistical,Outcome Assessment (Health Care),Parametric models,Public health. Hygiene,Public health. Hygiene-occupational medicine,Risk Factors,Term weighting,Unbiased estimators},
  language = {eng},
  number = {6}
}

@article{wangDiagnosingBiasInverse2006,
  title = {Diagnosing {{Bias}} in the {{Inverse Probability}} of {{Treatment Weighted Estimator Resulting}} from {{Violation}} of {{Experimental Treatment Assignment}}},
  author = {Wang, Yue and Petersen, Maya and Bangsberg, David and van der Laan, Mark},
  year = {2006},
  month = sep,
  volume = {Working Paper 211},
  file = {/Users/debaum/Zotero/storage/Z5TPV2V6/paper211.html},
  journal = {U.C. Berkeley Division of Biostatistics Working Paper Series}
}

@article{wangMinimalDispersionApproximately2020,
  title = {Minimal {{Dispersion Approximately Balancing Weights}}: {{Asymptotic Properties}} and {{Practical Considerations}}},
  shorttitle = {Minimal Dispersion Approximately Balancing Weights},
  author = {Wang, Yixin and Zubizarreta, Jose R.},
  year = {2020},
  volume = {107},
  pages = {93--105},
  publisher = {{Oxford University Press}},
  address = {{OXFORD}},
  issn = {0006-3444},
  doi = {10.1093/biomet/asz050},
  abstract = {Summary Weighting methods are widely used to adjust for covariates in observational studies, sample surveys, and regression settings. In this paper, we study a class of recently proposed weighting methods, which find the weights of minimum dispersion that approximately balance the covariates. We call these weights `minimal weights' and study them under a common optimization framework. Our key observation is that finding weights which achieve approximate covariate balance is equivalent to performing shrinkage estimation of the inverse propensity score. This connection leads to both theoretical and practical developments. From a theoretical standpoint, we characterize the asymptotic properties of minimal weights and show that, under standard smoothness conditions on the propensity score function, minimal weights are consistent estimates of the true inverse probability weights. In addition, we show that the resulting weighting estimator is consistent, asymptotically normal and semiparametrically efficient. From a practical standpoint, we give a finite-sample oracle inequality that bounds the loss incurred by balancing more functions of the covariates than strictly needed. This inequality shows that minimal weights implicitly bound the number of active covariate balance constraints. Finally, we provide a tuning algorithm for choosing the degree of approximate balance in minimal weights. The paper concludes with an empirical study which suggests that approximate balance is preferable to exact balance, especially when there is limited overlap in covariate distributions. Further studies show that the root mean squared error of the weighting estimator can be reduced by as much as a half with approximate balance.},
  file = {/Users/debaum/Zotero/storage/4K5SBLRM/Wang and Zubizarreta - 2020 - Minimal dispersion approximately balancing weights.pdf},
  journal = {Biometrika},
  keywords = {Biology,Life Sciences \& Biomedicine,Life Sciences \& Biomedicine - Other Topics,Mathematical \& Computational Biology,Mathematics,Physical Sciences,Science \& Technology,Statistics \& Probability},
  language = {eng},
  number = {1}
}

@article{wodtkeNeighborhoodEffectsTemporal2011,
  title = {Neighborhood {{Effects}} in {{Temporal Perspective}}: {{The Impact}} of {{Long}}-{{Term Exposure}} to {{Concentrated Disadvantage}} on {{High School Graduation}}},
  shorttitle = {Neighborhood {{Effects}} in {{Temporal Perspective}}},
  author = {Wodtke, Geoffrey T. and Harding, David J. and Elwert, Felix},
  year = {2011},
  volume = {76},
  pages = {713--736},
  publisher = {{Sage Publications}},
  address = {{Los Angeles, CA}},
  issn = {0003-1224},
  doi = {10.1177/0003122411420816},
  abstract = {Theory suggests that neighborhood effects depend not only on where individuals live today, but also on where they lived in the past. Previous research, however, usually measures neighborhood context only once and does not account for length of residence, thereby understating the detrimental effects of long-term neighborhood disadvantage. This study investigates effects of duration of exposure to disadvantaged neighborhoods on high school graduation. It follows 4,154 children in the Panel Study of Income Dynamics, measuring neighborhood context once per year from age 1 to 17. The analysis overcomes the problem of dynamic neighborhood selection by adapting novel methods of causal inference for time-varying treatments. In contrast to previous analyses, these methods do not "control away" the effect of neighborhood context operating indirectly through time-varying characteristics of the family; thus, they capture the full impact of a lifetime of neighborhood disadvantage. We find that sustained exposure to disadvantaged neighborhoods has a severe impact on high school graduation that is considerably larger than effects reported in prior research. We estimate that growing up in the most (compared to the least) disadvantaged quintile of neighborhoods reduces the probability of graduation from 96 to 76 percent for black children, and from 95 to 87 percent for nonblack children.},
  file = {/Users/debaum/Zotero/storage/6EKWC5Y2/Wodtke et al. - 2011 - Neighborhood Effects in Temporal Perspective The .pdf},
  journal = {American Sociological review},
  keywords = {Black people,Childhood,Children,Education; Secondary,Graduations,High school graduates,Influence,Neighborhood,Neighborhood conditions,Neighborhood schools,Neighborhoods,Observational research,Poverty,Regression analysis,Research,Social problems and social policy. Social work,Socially handicapped,Sociology,Sociology of education. Educational systems. Lifelong education},
  language = {eng},
  number = {5}
}

@article{zhaoCovariateBalancingPropensity2019,
  title = {Covariate {{Balancing Propensity Score}} by {{Tailored Loss Functions}}},
  author = {Zhao, Qingyuan},
  year = {2019},
  volume = {47},
  pages = {965-},
  publisher = {{Institute of Mathematical Statistics}},
  address = {{Hayward}},
  issn = {0090-5364},
  doi = {10.1214/18-AOS1698},
  abstract = {In observational studies, propensity scores are commonly estimated by maximum likelihood but may fail to balance high-dimensional pretreatment covariates even after specification search. We introduce a general framework that unifies and generalizes several recent proposals to improve covariate balance when designing an observational study. Instead of the likelihood function, we propose to optimize special loss functions-covariate balancing scoring rules (CBSR)-to estimate the propensity score. A CBSR is uniquely determined by the link function in the GLM and the estimand (a weighted average treatment effect). We show CBSR does not lose asymptotic efficiency in estimating the weighted average treatment effect compared to the Bernoulli likelihood, but CBSR is much more robust in finite samples. Borrowing tools developed in statistical learning, we propose practical strategies to balance covariate functions in rich function classes. This is useful to estimate the maximum bias of the inverse probability weighting (IPW) estimators and construct honest confidence intervals in finite samples. Lastly, we provide several numerical examples to demonstrate the tradeoff of bias and variance in the IPW-type estimators and the tradeoff in balancing different function classes of the covariates.},
  file = {/Users/debaum/Zotero/storage/7XYRPR23/Zhao - 2019 - Covariate balancing propensity score by tailored l.pdf},
  journal = {The Annals of statistics},
  keywords = {Asymptotic methods,Balancing,Bias,Confidence intervals,Estimating techniques,Estimators,Finite element analysis,Maximum likelihood estimates,Numerical analysis,Pretreatment,Robustness (mathematics),Statistical analysis,Statistical methods,Tradeoffs},
  language = {eng},
  number = {2}
}

@article{zhaoEntropyBalancingDoubly2016,
  title = {Entropy {{Balancing}} Is {{Doubly Robust}}},
  author = {Zhao, Qingyuan and Percival, Daniel},
  year = {2016},
  volume = {5},
  pages = {41--55},
  publisher = {{De Gruyter}},
  issn = {2193-3677},
  doi = {10.1515/jci-2016-0010},
  abstract = {Covariate balance is a conventional key diagnostic for methods estimating causal effects from observational studies. Recently, there is an emerging interest in directly incorporating covariate balance in the estimation. We study a recently proposed entropy maximization method called Entropy Balancing (EB), which exactly matches the covariate moments for the different experimental groups in its optimization problem. We show EB is doubly robust with respect to linear outcome regression and logistic propensity score regression, and it reaches the asymptotic semiparametric variance bound when both regressions are correctly specified. This is surprising to us because there is no attempt to model the outcome or the treatment assignment in the original proposal of EB. Our theoretical results and simulations suggest that EB is a very appealing alternative to the conventional weighting estimators that estimate the propensity score by maximum likelihood.},
  file = {/Users/debaum/Zotero/storage/ENP32S5C/Zhao and Percival - 2016 - Entropy Balancing is Doubly Robust.pdf},
  journal = {Journal of Causal Inference},
  keywords = {causal inference,convex optimization,double robustness,exponential tilting,survey sampling},
  language = {eng},
  number = {1}
}

@article{zhouRegressionwithResidualsMethodEstimating2019,
  title = {A {{Regression}}-with-{{Residuals Method}} for {{Estimating Controlled Direct Effects}}},
  author = {Zhou, Xiang and Wodtke, Geoffrey T.},
  year = {2019},
  volume = {27},
  pages = {360--369},
  publisher = {{Cambridge University Press}},
  address = {{New York, USA}},
  issn = {1047-1987},
  doi = {10.1017/pan.2018.53},
  abstract = {Political scientists are increasingly interested in causal mediation, and to this end, recent studies focus on estimating a quantity called the controlled direct effect (CDE). The CDE measures the strength of the causal relationship between a treatment and outcome when a mediator is fixed at a given value. To estimate the CDE, Joffe and Greene (2009) and Vansteelandt (2009) developed the method of sequential g-estimation, which was introduced to political science by Acharya, Blackwell, and Sen (2016). In this letter, we propose an alternative method called ``regression-with-residuals'' (RWR) for estimating the CDE. In special cases, we show that these two methods are algebraically equivalent. Yet, unlike sequential g-estimation, RWR can easily accommodate several types of effect moderation, including cases in which the effect of the mediator on the outcome is moderated by a posttreatment confounder. Although common in the social sciences, this type of effect moderation is typically assumed away in applications of sequential g-estimation, which may lead to bias if effect moderation is in fact present. We illustrate RWR by estimating the CDE of negative media framing on public support for immigration, controlling for respondent anxiety.},
  file = {/Users/debaum/Zotero/storage/HAB4HXHG/Zhou and Wodtke - 2019 - A Regression-with-Residuals Method for Estimating .pdf},
  journal = {Political Analysis},
  language = {eng},
  number = {3}
}

@article{zhouResidualBalancingMethod2020a,
  title = {Residual {{Balancing}}: {{A Method}} of {{Constructing Weights}} for {{Marginal Structural Models}}},
  shorttitle = {Residual {{Balancing}}},
  author = {Zhou, Xiang and Wodtke, Geoffrey T.},
  year = {2020},
  volume = {28},
  pages = {487--506},
  publisher = {{UNIV PRESS}},
  address = {{CAMBRIDGE}},
  issn = {1047-1987},
  doi = {10.1017/pan.2020.2},
  abstract = {When making causal inferences, post-treatment confounders complicate analyses of time-varying treatment effects. Conditioning on these variables naively to estimate marginal effects may inappropriately block causal pathways and may induce spurious associations between treatment and the outcome, leading to bias. To avoid such bias, researchers often use marginal structural models (MSMs) with inverse probability weighting (IPW). However, IPW requires models for the conditional distributions of treatment and is highly sensitive to their misspecification. Moreover, IPW is relatively inefficient, susceptible to finite-sample bias, and difficult to use with continuous treatments. We introduce an alternative method of constructing weights for MSMs, which we call "residual balancing". In contrast to IPW, it requires modeling the conditional means of the post-treatment confounders rather than the conditional distributions of treatment, and it is therefore easier to use with continuous treatments. Numeric simulations suggest that residual balancing is both more efficient and more robust to model misspecification than IPW and its variants in a variety of scenarios. We illustrate the method by estimating (a) the cumulative effect of negative advertising on election outcomes and (b) the controlled direct effect of shared democracy on public support for war. Open-source software is available for implementing the proposed method.},
  file = {/Users/debaum/Zotero/storage/6DU3C6FT/Zhou and Wodtke - 2020 - Residual Balancing A Method of Constructing Weigh.pdf},
  journal = {Political Analysis},
  keywords = {Bias,Computer software,Conditioning,Effects,Government \& Law,Mediators,Methods,Morality,Political advertising,Political Science,Public support,Social Sciences,Structural models,Variables,Weighting},
  language = {eng},
  number = {4}
}

@article{zhukovExternalResourcesIndiscriminate2017,
  title = {External {{Resources}} and {{Indiscriminate Violence}}: {{Evidence}} from {{German}}-{{Occupied Belarus}}},
  shorttitle = {External {{Resources}} and {{Indiscriminate Violence}}},
  author = {Zhukov, Yuri M.},
  year = {2017},
  volume = {69},
  pages = {54--97},
  publisher = {{Cambridge University Press}},
  issn = {0043-8871},
  doi = {10.1017/S0043887116000137},
  abstract = {Within a single conflict, the scale of government violence against civilians can vary greatly\textemdash from mass atrocities in one village to eerie restraint in the next. This article argues that the scale of anticivilian violence depends on a combatant's relative dependence on local and external sources of support. External resources make combatants less dependent on the local population, yet create perverse incentives for how the population is to be treated. Efforts by the opposition to interdict the government's external resources can reverse this effect, making the government more dependent on the local population. The article tests this relationship with disaggregated archival data on German-occupied Belarus during World War II. It finds that Soviet partisan attacks against German personnel provoked reprisals against civilians but that attacks against railroads had the opposite effect. Where partisans focused on disrupting German supply lines rather than killing Germans, occupying forces conducted fewer reprisals, burned fewer houses, and killed fewer people.},
  journal = {World Politics},
  keywords = {20th century,Belarus,German occupation; 1941-1944,History,Political violence},
  language = {eng},
  number = {1}
}

@article{zubizarretaStableWeightsThat2015,
  title = {Stable {{Weights}} That {{Balance Covariates}} for {{Estimation With Incomplete Outcome Data}}},
  author = {Zubizarreta, Jos{\'e} R.},
  year = {2015},
  volume = {110},
  pages = {910--922},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2015.1023805},
  abstract = {Weighting methods that adjust for observed covariates, such as inverse probability weighting, are widely used for causal inference and estimation with incomplete outcome data. Part of the appeal of such methods is that one set of weights can be used to estimate a range of treatment effects based on different outcomes, or a variety of population means for several variables. However, this appeal can be diminished in practice by the instability of the estimated weights and by the difficulty of adequately adjusting for observed covariates in some settings. To address these limitations, this article presents a new weighting method that finds the weights of minimum variance that adjust or balance the empirical distribution of the observed covariates up to levels prespecified by the researcher. This method allows the researcher to balance very precisely the means of the observed covariates and other features of their marginal and joint distributions, such as variances and correlations and also, for example, the quantiles of interactions of pairs and triples of observed covariates, thus, balancing entire two- and three-way marginals. Since the weighting method is based on a well-defined convex optimization problem, duality theory provides insight into the behavior of the variance of the optimal weights in relation to the level of covariate balance adjustment, answering the question, how much does tightening a balance constraint increases the variance of the weights? Also, the weighting method runs in polynomial time so relatively large datasets can be handled quickly. An implementation of the method is provided in the new package sbw for R. This article shows some theoretical properties of the resulting weights and illustrates their use by analyzing both a dataset from the 2010 Chilean earthquake and a simulated example.},
  journal = {Journal of the American Statistical Association},
  keywords = {Applications and Case Studies,Causal inference,Observational study,Propensity score,Quadratic programming,Unit nonresponse,Weight adjustment},
  language = {eng},
  number = {511}
}

@Manual{R-CBPS,
  title = {CBPS: Covariate Balancing Propensity Score},
  author = {Christian Fong and Marc Ratkovic and Kosuke Imai},
  year = {2021},
  note = {R package version 0.22},
  url = {https://CRAN.R-project.org/package=CBPS},
}

@Manual{R-ipw,
  title = {ipw: Estimate Inverse Probability Weights},
  author = {Ronald B. Geskus and Willem M. {van der Wal}},
  year = {2015},
  note = {R package version 1.0-11},
  url = {https://CRAN.R-project.org/package=ipw},
}

@Manual{R-ebal,
  title = {ebal: Entropy reweighting to create balanced samples},
  author = {Jens Hainmueller},
  year = {2014},
  note = {R package version 0.1-6},
  url = {https://CRAN.R-project.org/package=ebal},
}



